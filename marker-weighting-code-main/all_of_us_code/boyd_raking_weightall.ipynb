{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raking of the AllOfUs Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs raking according to David Marker's memo 5.4. It tries the univariate raking model and all models with pairwise interactions and scores each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure [repo](https://github.gatech.edu/rb230/t10-raking) is checked out to `/home/jupyter/workspaces/duplicateofgtrihealthanalytics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to True to use the imputed income data\n",
    "IMPUTE_INCOME = True\n",
    "\n",
    "# four-digit year of the PUMS target data, must already be recoded and collapsed\n",
    "PUMS_YEAR = 2020\n",
    "\n",
    "# directory where the source and target recoded input files are located\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "# threshold for auto-collapse of bins; set to 0 to disable\n",
    "AUTO_COLLAPSE_THRESHOLD = 0\n",
    "\n",
    "# minimum count in any cell of any source marginal distribution prior to raking\n",
    "MIN_CELL_SIZE = 50\n",
    "\n",
    "# which model to use for weighting (index into the MODELS list)\n",
    "# 0 : univariate\n",
    "# 1 : [45]\n",
    "# 2 : [35]\n",
    "MODEL_INDEX = 2\n",
    "\n",
    "# write result files for each state here\n",
    "# each state file has the form aou_xx_models.csv\n",
    "#OUTPUT_DIR = os.path.join('results', 'weighting_{0}'.format(PUMS_YEAR), 'model_{0}'.format(MODEL_INDEX))\n",
    "OUTPUT_DIR = os.path.join('results', 'weight_as_{0}'.format(PUMS_YEAR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output directory: {0}'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of User Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = '/home/jupyter/workspaces/duplicateofgtrihealthanalytics/t10-raking'\n",
    "assert os.path.exists(repo_dir)\n",
    "assert os.listdir(repo_dir)\n",
    "sys.path.insert(1, repo_dir)\n",
    "\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that some additional packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipfn\n",
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from src import plots, pdf, autocollapse, raking, census_divisions as census\n",
    "\n",
    "# coding file - ensure that the input files were generated with the selected coding scheme\n",
    "from src import coding_aou as CODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the AllOfUs Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPUTE_INCOME:\n",
    "    name_of_file_in_bucket = 'AoU_Impute.csv'\n",
    "else:\n",
    "    name_of_file_in_bucket = 'baseline_population_v5.csv'\n",
    "    \n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "os.system(f\"gsutil cp '{my_bucket}/data/{name_of_file_in_bucket}' .\")\n",
    "\n",
    "print(f'[INFO] {name_of_file_in_bucket} is successfully downloaded into your working space')\n",
    "baseline = pd.read_csv(name_of_file_in_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in baseline.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Recoded PUMS USA File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recoded PUMS file for all 50 states and DC\n",
    "PUMS_RECODED_FILE_NAME = 'pums_usa_{0}_recoded_aou.csv'.format(PUMS_YEAR)\n",
    "\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "#aou_pums = 'data/pums_usa_{0}_recoded_aou.csv'.format(PUMS_YEAR)\n",
    "aou_pums = os.path.join(DATA_DIR, PUMS_RECODED_FILE_NAME)\n",
    "with open(aou_pums, 'w') as f:\n",
    "    if 2019 == PUMS_YEAR:\n",
    "        res = requests.get('https://gtri.box.com/shared/static/lonqp4hcbdr1wk9anhd7obbqbqxz8vww.csv')\n",
    "    elif 2020 == PUMS_YEAR:\n",
    "        res = requests.get('https://gtri.box.com/shared/static/lwgs9nl7b9hw7h4e1niatszvu4oq3ulx.csv')\n",
    "        \n",
    "    f.write(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifiers for the variables to be raked (from the coding file) - also specifies their ordering.\n",
    "# *** The ordering of the variables affects the raking metric. ***\n",
    "\n",
    "# for imputed income file\n",
    "if IMPUTE_INCOME:\n",
    "    RAKE_DATA = [\n",
    "        # variable enum,              # recoded source var name,   # recoded PUMS var name\n",
    "        (CODING.Variables.AGE,        'AGEc',                      'Age'),          \n",
    "#         (CODING.Variables.RACE_ETH,   'RACE_GROUPING',             'RaceEth'),\n",
    "#         (CODING.Variables.INSURANCE,  'INSURANCE_GROUPING',        'Insurance'),\n",
    "#         (CODING.Variables.EDUCATION,  'EDUCATION_GROUPING',        'Education'),\n",
    "#         (CODING.Variables.INCOME,     'INCOME_NUM',                'Income'),  \n",
    "        (CODING.Variables.SEX,        'sg',                        'Sex'),        \n",
    "    ]\n",
    "else:\n",
    "    RAKE_DATA = [\n",
    "        # variable enum,              # recoded source var name,   # recoded PUMS var name\n",
    "        (CODING.Variables.AGE,        'AGE_GROUPING',              'Age'),          \n",
    "#         (CODING.Variables.RACE_ETH,   'RACE_GROUPING',             'RaceEth'),\n",
    "#         (CODING.Variables.INSURANCE,  'INSURANCE_GROUPING',        'Insurance'),\n",
    "#         (CODING.Variables.EDUCATION,  'EDUCATION_GROUPING',        'Education'),\n",
    "#         (CODING.Variables.INCOME,     'INCOME_GROUPING',           'Income'),  \n",
    "        (CODING.Variables.SEX,        'SEX_AT_BIRTH',              'Sex'),        \n",
    "    ]    \n",
    "\n",
    "# map of the enumvar to (source, target) colum name tuple\n",
    "RAKE_MAP = {enumvar:(source_col, target_col) for enumvar, source_col, target_col in RAKE_DATA}\n",
    "    \n",
    "\n",
    "\n",
    "# directory where the source and target recoded input files are located\n",
    "#DATA_DIR = 'data'\n",
    "\n",
    "# name of the recoded NHIS data\n",
    "#AOU_RECODED_FILE_NAME = 'synthetic_aou.csv'\n",
    "\n",
    "# Whether to use weighted PUMS counts, which are required for true population counts (PUMS 'PWGTP' variable).\n",
    "# Set to None to use an unweighted target population.\n",
    "PUMS_WEIGHT_COL = 'PWGTP'\n",
    "\n",
    "# source state two-letter abbreviation or abbreviation for census division (listed below)\n",
    "# census divisions: DIV_NE, DIV_MA, DIV_ENC, DIV_WNC, DIV_SA, DIV_ESC, DIV_WSC, DIV_M, DIV_P\n",
    "#SOURCE_STATE = 'DIV_P'\n",
    "\n",
    "# PUMS target state abbreviation\n",
    "# (if SOURCE_STATE is a census division, the target state must also be in that division)\n",
    "#TARGET_STATE = 'AK'\n",
    "\n",
    "# a weighted version of the source file will be written to DATA_DIR\n",
    "#OUTPUT_FILE_NAME = 'aou_weighted_{0}_{1}.csv'.format(SOURCE_STATE, TARGET_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup some data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map of state abbreviations to PUMS state codes\n",
    "STATE_CODE_MAP = {\n",
    "    'AL' : 1,\n",
    "    'AK' : 2,\n",
    "    'AZ' : 4,\n",
    "    'AR' : 5,\n",
    "    'CA' : 6,\n",
    "    'CO' : 8,\n",
    "    'CT' : 9,\n",
    "    'DE' : 10,\n",
    "    'DC' : 11,\n",
    "    'FL' : 12,\n",
    "    'GA' : 13,\n",
    "    'HI' : 15,\n",
    "    'ID' : 16,\n",
    "    'IL' : 17,\n",
    "    'IN' : 18,\n",
    "    'IA' : 19,\n",
    "    'KS' : 20,\n",
    "    'KY' : 21,\n",
    "    'LA' : 22,\n",
    "    'ME' : 23,\n",
    "    'MD' : 24,\n",
    "    'MA' : 25,\n",
    "    'MI' : 26,\n",
    "    'MN' : 27,\n",
    "    'MS' : 28,\n",
    "    'MO' : 29,\n",
    "    'MT' : 30,\n",
    "    'NE' : 31,\n",
    "    'NV' : 32,\n",
    "    'NH' : 33,\n",
    "    'NJ' : 34,\n",
    "    'NM' : 35,\n",
    "    'NY' : 36,\n",
    "    'NC' : 37,\n",
    "    'ND' : 38,\n",
    "    'OH' : 39,\n",
    "    'OK' : 40,\n",
    "    'OR' : 41,\n",
    "    'PA' : 42,\n",
    "    'RI' : 44,\n",
    "    'SC' : 45,\n",
    "    'SD' : 46,\n",
    "    'TN' : 47,\n",
    "    'TX' : 48,\n",
    "    'UT' : 49,\n",
    "    'VT' : 50,\n",
    "    'VA' : 51,\n",
    "    'WA' : 53,\n",
    "    'WV' : 54,\n",
    "    'WI' : 55,\n",
    "    'WY' : 56,\n",
    "}\n",
    "\n",
    "INV_STATE_CODE_MAP = {v:k for k,v in STATE_CODE_MAP.items()}\n",
    "\n",
    "# map of state abbreviations to its US Census division\n",
    "CENSUS_DIVISION_MAP = {\n",
    "    'AL' : 'DIV_ESC',\n",
    "    'AK' : 'DIV_P',\n",
    "    'AZ' : 'DIV_M',\n",
    "    'AR' : 'DIV_WSC',\n",
    "    'CA' : 'DIV_P',\n",
    "    'CO' : 'DIV_M',\n",
    "    'CT' : 'DIV_NE',\n",
    "    'DE' : 'DIV_SA',\n",
    "    'DC' : 'DIV_SA',\n",
    "    'FL' : 'DIV_SA',\n",
    "    'GA' : 'DIV_SA',\n",
    "    'HI' : 'DIV_P',\n",
    "    'ID' : 'DIV_M',\n",
    "    'IL' : 'DIV_ENC',\n",
    "    'IN' : 'DIV_ENC',\n",
    "    'IA' : 'DIV_WNC',\n",
    "    'KS' : 'DIV_WNC',\n",
    "    'KY' : 'DIV_ESC',\n",
    "    'LA' : 'DIV_WSC',\n",
    "    'ME' : 'DIV_NE',\n",
    "    'MD' : 'DIV_SA',\n",
    "    'MA' : 'DIV_NE',\n",
    "    'MI' : 'DIV_ENC',\n",
    "    'MN' : 'DIV_WNC',\n",
    "    'MS' : 'DIV_ESC',\n",
    "    'MO' : 'DIV_WNC',\n",
    "    'MT' : 'DIV_M',\n",
    "    'NE' : 'DIV_WNC',\n",
    "    'NV' : 'DIV_M',\n",
    "    'NH' : 'DIV_NE',\n",
    "    'NJ' : 'DIV_MA',\n",
    "    'NM' : 'DIV_M',\n",
    "    'NY' : 'DIV_MA',\n",
    "    'NC' : 'DIV_SA',\n",
    "    'ND' : 'DIV_WNC',\n",
    "    'OH' : 'DIV_ENC',\n",
    "    'OK' : 'DIV_WSC',\n",
    "    'OR' : 'DIV_P',\n",
    "    'PA' : 'DIV_MA',\n",
    "    'RI' : 'DIV_NE',\n",
    "    'SC' : 'DIV_SA',\n",
    "    'SD' : 'DIV_WNC',\n",
    "    'TN' : 'DIV_ESC',\n",
    "    'TX' : 'DIV_WSC',\n",
    "    'UT' : 'DIV_M',\n",
    "    'VT' : 'DIV_NE',\n",
    "    'VA' : 'DIV_SA',\n",
    "    'WA' : 'DIV_P',\n",
    "    'WV' : 'DIV_SA',\n",
    "    'WI' : 'DIV_ENC',\n",
    "    'WY' : 'DIV_M',\n",
    "}\n",
    "\n",
    "# the seventeen states with 1K or more AllOfUs samples\n",
    "STATES_1K = {\n",
    "    'CA', 'AZ', 'IL', 'PA', 'NY', 'MA', 'AL',\n",
    "    'WI', 'MI', 'FL', 'TX', 'GA', 'LA', 'MS',\n",
    "    'SC', 'TN', 'CT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of enumerated variables to be raked\n",
    "RAKEVARS = [tup[0] for tup in RAKE_DATA]\n",
    "\n",
    "# map of variable enum to source file col name\n",
    "AOU_COL_MAP = {RAKEVARS[i]:RAKE_DATA[i][1] for i in range(len(RAKEVARS))}\n",
    "\n",
    "# map of variable enum to PUMS file col name\n",
    "PUMS_COL_MAP = {RAKEVARS[i]:RAKE_DATA[i][2] for i in range(len(RAKEVARS))}\n",
    "\n",
    "# get the names of the variables, in order\n",
    "RAKEVAR_NAMES = [CODING.VAR_NAMES[enumvar] for enumvar in RAKEVARS]\n",
    "RAKEVAR_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose list of models based on the problem dimension\n",
    "dim = len(RAKEVARS)\n",
    "\n",
    "if 2 == dim:\n",
    "    MODELS = raking.MODELS_2\n",
    "elif 3 == dim:\n",
    "    MODELS = raking.MODELS_3\n",
    "elif 4 == dim:\n",
    "    MODELS = raking.MODELS_4\n",
    "elif 5 == dim:\n",
    "    MODELS = raking.MODELS_5\n",
    "elif 6 == dim:\n",
    "    MODELS = raking.MODELS_6\n",
    "else:\n",
    "    # just use 1D marginals\n",
    "    MODELS = [\n",
    "        [[i] for i in range(dim)],\n",
    "    ]\n",
    "    \n",
    "RAKING_MODEL = MODELS[MODEL_INDEX]\n",
    "print('Using model \"{0}\"'.format(RAKING_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the PUMS data for the USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recoded PUMS file for the entire USA\n",
    "TARGET_FILE = os.path.join(DATA_DIR, PUMS_RECODED_FILE_NAME)\n",
    "print('Target file: {0}'.format(TARGET_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading target file \"{0}\"...'.format(TARGET_FILE))\n",
    "pums_usa_df = pd.read_csv(TARGET_FILE)\n",
    "pums_usa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract PUMS data for the target state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pums_state_data(pums_usa_df, state_abbrev):\n",
    "    \n",
    "    # extract PUMS data for the specified state\n",
    "    #assert TARGET_STATE in STATE_CODE_MAP\n",
    "    assert state_abbrev in STATE_CODE_MAP\n",
    "    state_code = STATE_CODE_MAP[state_abbrev]\n",
    "\n",
    "    # extract all rows for this state\n",
    "    pums_df = pums_usa_df.loc[pums_usa_df['ST'] == state_code]\n",
    "\n",
    "    # keep specified data cols and weight col, drop all others\n",
    "    keep_cols = [col for col in PUMS_COL_MAP.values()]\n",
    "    keep_cols.append(PUMS_WEIGHT_COL)\n",
    "    pums_df = pums_df[keep_cols]\n",
    "    pums_df = pums_df.reset_index(drop=True)\n",
    "    return pums_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the AllOfUs Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_source_df = baseline\n",
    "# extract required cols\n",
    "cols = [c for c in AOU_COL_MAP.values()]\n",
    "cols.append('STATE')\n",
    "cols.append('person_id')\n",
    "raw_source_df = baseline[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_data(raw_source_df, state_abbrev):\n",
    "\n",
    "    if state_abbrev in STATES_1K:\n",
    "        # single state\n",
    "        state_set = {state_abbrev}\n",
    "    else:\n",
    "        assert state_abbrev in CENSUS_DIVISION_MAP\n",
    "        division = CENSUS_DIVISION_MAP[state_abbrev]\n",
    "        state_tuples = census.CENSUS_DIV_MAP[division]\n",
    "        state_set = {abbrev for abbrev, code in state_tuples}\n",
    "        print('States in census division {0}: {1}'.format(division, sorted(list(state_set))))\n",
    "        \n",
    "#     if 2 == len(SOURCE_STATE):\n",
    "#         # single state\n",
    "#         state_set = {SOURCE_STATE}\n",
    "#     else:\n",
    "#         # census division\n",
    "#         assert SOURCE_STATE in census.CENSUS_DIV_MAP\n",
    "#         state_tuples = census.CENSUS_DIV_MAP[SOURCE_STATE]\n",
    "#         state_set = {state_abbrev for state_abbrev, state_code in state_tuples}\n",
    "#         print('States in census division {0}: {1}'.format(SOURCE_STATE, sorted(list(state_set))))\n",
    "\n",
    "#         if TARGET_STATE not in state_set:\n",
    "#             raise ValueError('Target state not in specified census division.')\n",
    "\n",
    "    # extract data for desired state(s)\n",
    "    source_df = raw_source_df[raw_source_df['STATE'].isin(state_set)]\n",
    "\n",
    "    source_df = source_df.reset_index(drop=True)\n",
    "    print('States in dataframe: {0}'.format(sorted(list(source_df['STATE'].unique()))))\n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recode the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPUTE_INCOME:\n",
    "    # for imputed income file\n",
    "    age_recode  = {\n",
    "        '18-29':0, # 20-29 => 18_29\n",
    "        '30-39':1, # 30-39 => 30_39\n",
    "        '40-49':2, \n",
    "        '50-59':3, \n",
    "        '60-69':4, \n",
    "        '70+'  :5, # 70-79 => 70_plus\n",
    "        '70+'  :5, # 80-89 => 70_plus\n",
    "        '70+'  :5, # 90-99 => 70_plus\n",
    "    }\n",
    "else:\n",
    "    age_recode  = {\n",
    "        '20-29':0, # 20-29 => 18_29\n",
    "        '30-39':1, # 30-39 => 30_39\n",
    "        '40-49':2, \n",
    "        '50-59':3, \n",
    "        '60-69':4, \n",
    "        '70-79':5, # 70-79 => 70_plus\n",
    "        '80-89':5, # 80-89 => 70_plus\n",
    "        '90-99':5, # 90-99 => 70_plus\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_recode = {\n",
    "    'NH White':0, # NH White => NH White\n",
    "    'NH Black':1, # NH Black => NH Black\n",
    "    'NH Asian':2, # NH Asian => NH Asian\n",
    "    'Hispanic':3, # Hispanic => Hispanic\n",
    "    'Other'   :4, # Other => Other\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_recode = {\n",
    "    'Male'  :0, # Male => Male\n",
    "    'Female':1, # Female => Female\n",
    "    'Other' :0, # Other => Male\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_recode = {\n",
    "    'College Grad'    :0, # College Grad => College Grad\n",
    "    'Some College'    :1, # Some College => Some College\n",
    "    'High School Grad':2, # HS Grad => HS Grad\n",
    "    'Not Grad'        :3, # Not HS Grad => Not HS Grad\n",
    "    'Missing'         :0, # Missing => College Grad\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_recode = {\n",
    "    'Yes'    :0, # Yes => Yes\n",
    "    'No'     :1, # No => No\n",
    "    'Missing':1, # Missing => No\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPUTE_INCOME:\n",
    "    income_recode = {\n",
    "        1:0, # less than $25K => less than $25K\n",
    "        2:1, # [$25K, $50K) => [$25K, $50K)\n",
    "        3:2, # [$50K, $100K) => [$50K, $100K)\n",
    "        4:3, # $100K or more => $100K or more\n",
    "    }\n",
    "else:\n",
    "    income_recode = {\n",
    "        '<25K'    :0, # less than $25K => less than $25K\n",
    "        '25-50K'  :1, # [$25K, $50K) => [$25K, $50K)\n",
    "        '50-100K:':2, # [$50K, $100K) => [$50K, $100K)\n",
    "        '>100K'   :3, # $100K or more => $100K or more\n",
    "        'Missing' :4, # Missing => Missing\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each variable to its value map\n",
    "RECODE_MAP = {\n",
    "    CODING.Variables.AGE       : age_recode,\n",
    "    CODING.Variables.RACE_ETH  : race_recode,\n",
    "    CODING.Variables.SEX       : sex_recode,\n",
    "    CODING.Variables.EDUCATION : education_recode,\n",
    "    CODING.Variables.INCOME    : income_recode,\n",
    "    CODING.Variables.INSURANCE : insurance_recode\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recode the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_source(source_df):\n",
    "    for enumvar, tup in RAKE_MAP.items():\n",
    "        source_col, target_col = tup\n",
    "        value_map = RECODE_MAP[enumvar]\n",
    "        recoded_values = source_df[source_col].map(value_map)\n",
    "        source_df = source_df.assign(**{source_col:recoded_values})\n",
    "        \n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State-Specific Collapsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLAPSE AGE\n",
    "def collapse_age(source_df):\n",
    "    source_age_col, target_age_col = RAKE_MAP[CODING.Variables.AGE]\n",
    "    source_df = CODING.collapse_age(source_df, 'STATE', source_age_col)\n",
    "\n",
    "    # check\n",
    "\n",
    "    # Connecticut, state code 9\n",
    "    check_values = source_df[source_df['STATE']=='CT'][source_age_col].values\n",
    "    ctr = Counter(check_values)\n",
    "    assert 0 not in ctr.keys()\n",
    "    # South Carolina, state code 45\n",
    "    check_values = source_df[source_df['STATE']=='SC'][source_age_col].values\n",
    "    ctr = Counter(check_values)\n",
    "    assert 5 not in ctr.keys()\n",
    "    # States in the West North Central census division\n",
    "    check_values = source_df[source_df['STATE'].isin({'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD'})][source_age_col].values\n",
    "    ctr = Counter(check_values)\n",
    "    assert 3 not in ctr.keys()\n",
    "    assert 4 not in ctr.keys()\n",
    "    assert 5 not in ctr.keys()\n",
    "    \n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLAPSE RACE\n",
    "def collapse_race(source_df):\n",
    "    source_race_col, target_race_col = RAKE_MAP[CODING.Variables.RACE_ETH]\n",
    "    source_df = CODING.collapse_raceeth(source_df, 'STATE', source_race_col)\n",
    "\n",
    "    # check\n",
    "\n",
    "    # CT, TN, DIV_ESC, DIV_WSC\n",
    "    check_values = source_df[source_df['STATE'].isin({'CT', 'TN', 'AL', 'KY', 'MS', \\\n",
    "                                                      'TN', 'AR', 'LA', 'OK', 'TX'})][source_race_col].values\n",
    "    ctr = Counter(check_values)\n",
    "    assert 2 not in ctr.keys()\n",
    "    # South Carolina, state code 45\n",
    "    check_values = source_df[source_df['STATE']=='SC'][source_race_col].values\n",
    "    ctr = Counter(check_values)\n",
    "    assert 2 not in ctr.keys()\n",
    "    assert 4 not in ctr.keys()\n",
    "    # LA and MS, codes 22 and 28\n",
    "    check_values = source_df[source_df['STATE'].isin({'LA', 'MS'})][source_race_col].values\n",
    "    ctr = Counter(check_values)\n",
    "    assert 2 not in ctr.keys()\n",
    "    assert 3 not in ctr.keys()\n",
    "    \n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLAPSE EDUCATION\n",
    "def collapse_education(source_df):\n",
    "    source_edu_col, target_edu_col = RAKE_MAP[CODING.Variables.EDUCATION]\n",
    "    source_df = CODING.collapse_education(source_df, 'STATE', source_edu_col)\n",
    "\n",
    "    # check\n",
    "\n",
    "    # DIV_WNC\n",
    "    check_values = source_df[source_df['STATE'].isin({'IA', 'KS', 'MN', 'MO', \n",
    "                                                      'NE', 'ND', 'SD'})][source_edu_col].values\n",
    "    ctr = Counter(check_values)\n",
    "    assert 3 not in ctr.keys()\n",
    "    \n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLAPSE INCOME\n",
    "def collapse_income(source_df):\n",
    "    source_income_col, target_income_col = RAKE_MAP[CODING.Variables.INCOME]\n",
    "\n",
    "    source_df = CODING.collapse_income(source_df, 'STATE', source_income_col)\n",
    "\n",
    "    # CT, MS, TN, and SC\n",
    "    check_values = source_df[source_df['STATE'].isin({'CT', 'MS', 'TN', 'SC'})][source_income_col].values\n",
    "    ctr = Counter(check_values)\n",
    "    assert 3 not in ctr.keys()\n",
    "    # DIV_WNC\n",
    "    check_values = source_df[source_df['STATE'].isin({'IA', 'KS', 'MN', 'MO', \n",
    "                                                      'NE', 'ND', 'SD'})][source_income_col].values\n",
    "    ctr = Counter(check_values)\n",
    "    assert 1 not in ctr.keys()\n",
    "    \n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_variables(df):\n",
    "    \n",
    "    if CODING.Variables.AGE in RAKE_MAP:\n",
    "        df = collapse_age(df)\n",
    "    if CODING.Variables.RACE_ETH in RAKE_MAP:\n",
    "        df = collapse_race(df)\n",
    "    if CODING.Variables.EDUCATION in RAKE_MAP:\n",
    "        df = collapse_education(df)\n",
    "    if CODING.Variables.INCOME in RAKE_MAP:\n",
    "        df = collapse_income(df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Collapsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform auto-collapsing based on bin counts. Bins with counts below a parameterized threshold are collapsed into an adjacent bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ORDERED_VARIABLES = {CODING.Variables.AGE, CODING.Variables.EDUCATION, CODING.Variables.INCOME}\n",
    "\n",
    "# # South Carolina changes the \"other\" bin for RaceEth\n",
    "# AUTO_COLLAPSE_RACE_OTHER = CODING.RaceEth.OTHER.value\n",
    "# if 'SC' == SOURCE_STATE and 'SC' == TARGET_STATE:\n",
    "#     AUTO_COLLAPSE_RACE_OTHER = 0\n",
    "\n",
    "# for enumvar, source_col, target_col in RAKE_DATA:\n",
    "    \n",
    "#     # print distributions prior to auto-collapse\n",
    "#     print('Variable: {0}'.format(enumvar))\n",
    "#     print('BEFORE: ')\n",
    "#     ctr = Counter(source_df[source_col].values)\n",
    "#     for k in sorted(ctr.keys()):\n",
    "#         print('{0} => {1}'.format(k, ctr[k]))\n",
    "#     ctr = Counter(pums_df[target_col].values)\n",
    "#     for k in sorted(ctr.keys()):\n",
    "#         print('\\t{0} => {1}'.format(k, ctr[k]))\n",
    "#     print()    \n",
    "    \n",
    "#     if enumvar in ORDERED_VARIABLES:\n",
    "                \n",
    "#         source_df, change_list = autocollapse.full_collapse_ordered(source_df,\n",
    "#                                                                     source_col,\n",
    "#                                                                     AUTO_COLLAPSE_THRESHOLD)\n",
    "#         pums_df = autocollapse.collapse_from_changelist(pums_df, target_col, change_list)\n",
    "        \n",
    "#         print('\\tchange_list {0}: {1}'.format(enumvar, change_list)) \n",
    "        \n",
    "#     elif enumvar == CODING.Variables.RACE_ETH:\n",
    "#         # unordered\n",
    "#         source_df, change_list = autocollapse.full_collapse_unordered(source_df,\n",
    "#                                                                       source_col,\n",
    "#                                                                       threshold = AUTO_COLLAPSE_THRESHOLD,\n",
    "#                                                                       other_col = AUTO_COLLAPSE_RACE_OTHER)\n",
    "#         pums_df = autocollapse.collapse_from_changelist(pums_df, target_col, change_list)\n",
    "        \n",
    "#         print('\\tchange_list {0}: {1}'.format(enumvar, change_list)) \n",
    "        \n",
    "#     else:\n",
    "#         # TBD for insurance, sex\n",
    "#         pass\n",
    "        \n",
    "#     # print distributions after auto-collapse\n",
    "#     print('AFTER: ')\n",
    "#     ctr = Counter(source_df[source_col].values)\n",
    "#     for k in sorted(ctr.keys()):\n",
    "#         print('{0} => {1}'.format(k, ctr[k]))\n",
    "#     ctr = Counter(pums_df[target_col].values)\n",
    "#     for k in sorted(ctr.keys()):\n",
    "#         print('\\t{0} => {1}'.format(k, ctr[k]))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build target sample arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_samples(pums_df):\n",
    "    # get the names of the target columns in the same order as the RAKEVARS array\n",
    "    ordered_target_cols = [PUMS_COL_MAP[enumvar] for enumvar in RAKEVARS]\n",
    "    print('Target columns, in order: {0}\\n'.format(ordered_target_cols))\n",
    "\n",
    "    # samples taken in order of the variables in VARIABLES\n",
    "    TARGET_SAMPLES = [np.array(pums_df[col].values) for col in ordered_target_cols]\n",
    "\n",
    "    # set TARGET_WEIGHTS to None to rake to an unweighted target population\n",
    "    if PUMS_WEIGHT_COL is not None:\n",
    "        TARGET_WEIGHTS = np.array(pums_df[PUMS_WEIGHT_COL].values)\n",
    "        TARGET_POPULATION = np.sum(TARGET_WEIGHTS)\n",
    "        print('Target population (weighted): {0}'.format(TARGET_POPULATION))\n",
    "    else:\n",
    "        TARGET_WEIGHTS = None\n",
    "        TARGET_POPULATION = len(TARGET_SAMPLES[0])    \n",
    "        print('Target population (unweighted): {0}'.format(TARGET_POPULATION))\n",
    "        \n",
    "    return TARGET_SAMPLES, TARGET_WEIGHTS, TARGET_POPULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build source sample arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_samples(source_df):\n",
    "    # get the names of the source dataframe columns in the order matching the variables\n",
    "    ordered_source_cols = [AOU_COL_MAP[enumvar] for enumvar in RAKEVARS]\n",
    "    print('Source columns, in order: {0}\\n'.format(ordered_source_cols))\n",
    "\n",
    "    # build a list of np.arrays containing the data for each col\n",
    "    SOURCE_SAMPLES = [np.array(source_df[col].values) for col in ordered_source_cols]\n",
    "\n",
    "    print('Sample population: {0}'.format(len(SOURCE_SAMPLES[0])))\n",
    "    \n",
    "    return SOURCE_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the number of bins required to hold each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical values for a given variable are **assumed** to be a contiguous block of integers starting at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bin counts in order of the variables\n",
    "BIN_COUNTS = []\n",
    "\n",
    "for enumvar in RAKEVARS:\n",
    "    BIN_COUNTS.append(CODING.BIN_COUNTS[enumvar])\n",
    "\n",
    "# maximum-length variable name, used for prettyprinting\n",
    "maxlen = max([len(var_name) for var_name in RAKEVAR_NAMES])\n",
    "\n",
    "print('Bin counts: ')\n",
    "for i in range(len(RAKEVAR_NAMES)):\n",
    "    print('{0:>{2}} : {1}'.format(RAKEVAR_NAMES[i], BIN_COUNTS[i], maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful plotting function\n",
    "def plot_pdfs(source_unraked_pdfs, source_raked_pdfs, target_pdfs, source_state, target_state):\n",
    "    \n",
    "    # display precision\n",
    "    P = 5\n",
    "    \n",
    "    num_pdfs = len(source_unraked_pdfs)\n",
    "    assert len(source_raked_pdfs) == len(target_pdfs) == num_pdfs\n",
    "    \n",
    "    for q in range(0, num_pdfs):\n",
    "        plots.triple_histogram_from_pdfs('{0} ({1}-{2})'.format(RAKEVAR_NAMES[q], source_state, target_state), \n",
    "                                         source_unraked_pdfs[q], source_raked_pdfs[q], target_pdfs[q],\n",
    "                                         labels=['Unraked Source', 'Raked Source', 'Target'])\n",
    "        print('Unraked source PDF : {0}'.format(np.array_str(source_unraked_pdfs[q], precision=P)))\n",
    "        print('  Raked source PDF : {0}'.format(np.array_str(source_raked_pdfs[q],   precision=P)))\n",
    "        print('        Target PDF : {0}'.format(np.array_str(target_pdfs[q],         precision=P)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raking in N Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score1(weights_m, sample_count):\n",
    "    \"\"\"\n",
    "    The original variability score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # max weight\n",
    "    wmax = np.max(weights_m)\n",
    "    \n",
    "    # 99th percentile weight\n",
    "    w99 = np.percentile(weights_m, 99)\n",
    "    \n",
    "    # find all weights >= w99\n",
    "    samples = []\n",
    "    for w in weights_m:\n",
    "        if w >= w99:\n",
    "            samples.append(w)\n",
    "    \n",
    "    # std deviation of these weights\n",
    "    std = np.std(samples)\n",
    "    \n",
    "    # the score is the product of the std deviation and the normalized max weight\n",
    "    # (idea is that a model that minimizes both is better)\n",
    "    score = (wmax / sample_count)  * std\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score2(weights_m, avg_wt):\n",
    "    \"\"\"\n",
    "    David Marker's modification of the variability score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # will sum all weights >= this fraction * max_wt\n",
    "    MAX_WT_FRAC = 0.9\n",
    "    \n",
    "    # maximum weight for this model\n",
    "    wmax = np.max(weights_m)\n",
    "    \n",
    "    # cutoff value; compute the sum of all weights >= this cutoff\n",
    "    w_cutoff = MAX_WT_FRAC * wmax\n",
    "    \n",
    "    samples = []\n",
    "    for w in weights_m:\n",
    "        if w >= w_cutoff:\n",
    "            samples.append(w)\n",
    "            \n",
    "    w_sum = np.sum(samples)\n",
    "    \n",
    "    # w_sum represents this percent of the weighted total population\n",
    "    pct = w_sum / TARGET_POPULATION\n",
    "     \n",
    "    # the score is the product of the max wt and the fraction above the cutoff, divided by the avg weight\n",
    "    score = wmax * pct / avg_wt\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_results(data):\n",
    "\n",
    "    sample_count = len(SOURCE_SAMPLES[0])\n",
    "    avg_wt = TARGET_POPULATION / sample_count\n",
    "\n",
    "    # compute a score for each model\n",
    "    scores = []\n",
    "    for wmax, smallest_cell, model_index in data:\n",
    "        model = MODELS[model_index]\n",
    "        weights_m = weight_map[model_index]\n",
    "        score = score2(weights_m, avg_wt)\n",
    "        scores.append(score)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for state_index, state_abbrev in enumerate(STATE_CODE_MAP):\n",
    " \n",
    "    # get target dataframe\n",
    "    pums_df = get_pums_state_data(pums_usa_df, state_abbrev)\n",
    "\n",
    "    # get source dataframe (could be multiple states)\n",
    "    source_df = get_source_data(raw_source_df, state_abbrev)\n",
    "\n",
    "    # recode the AllOfUs data to the scheme in the CODING file\n",
    "    # (PUMS has already been recoded to reduce the runtime)\n",
    "    source_df = recode_source(source_df)\n",
    "\n",
    "    # collapse the AllOfUs variables\n",
    "    # (PUMS has already been collapsed to reduce the runtime)\n",
    "    source_df = collapse_variables(source_df)\n",
    "\n",
    "    # get source and target sample arrays\n",
    "    TARGET_SAMPLES, TARGET_WEIGHTS, TARGET_POPULATION = get_target_samples(pums_df)\n",
    "    SOURCE_SAMPLES = get_source_samples(source_df)\n",
    "        \n",
    "    raking_result_tuple = raking.rake(RAKING_MODEL,\n",
    "                                      SOURCE_SAMPLES,\n",
    "                                      TARGET_SAMPLES,\n",
    "                                      TARGET_WEIGHTS,\n",
    "                                      BIN_COUNTS,\n",
    "                                      TARGET_POPULATION)\n",
    "\n",
    "    if raking_result_tuple is None:\n",
    "        print('\\n*** No acceptable raking model was found. ***')\n",
    "    else:\n",
    "        weights, unraked, target, raked, fnorm, smallest_cell = raking_result_tuple\n",
    "        # compute PDFs\n",
    "        target_pdfs = {}\n",
    "        source_raked_pdfs = {}\n",
    "        source_unraked_pdfs = {}\n",
    "\n",
    "        # target pdfs\n",
    "        for q in range(len(RAKEVARS)):\n",
    "            target_pdfs[q] = pdf.to_pdf(BIN_COUNTS[q], TARGET_SAMPLES[q], weights=TARGET_WEIGHTS)\n",
    "\n",
    "        # unraked source pdfs\n",
    "        for q in range(len(RAKEVARS)):\n",
    "            source_unraked_pdfs[q] = pdf.to_pdf(BIN_COUNTS[q], SOURCE_SAMPLES[q], weights=None)\n",
    "\n",
    "        # raked source pdfs\n",
    "        for q in range(len(RAKEVARS)):\n",
    "            source_raked_pdfs[q] = pdf.to_pdf(BIN_COUNTS[q], SOURCE_SAMPLES[q], weights=weights)\n",
    "\n",
    "        print('\\nPDF diffs after raking for model {0}:\\n'.format(RAKING_MODEL))   \n",
    "\n",
    "        # diff between raked and target pdfs\n",
    "        diffs = []\n",
    "        for q in range(len(RAKEVARS)):\n",
    "            diff = source_raked_pdfs[q] - target_pdfs[q]\n",
    "            diffs.append(diff)\n",
    "            print('{0:>{2}} : {1}'.format(RAKEVAR_NAMES[q], \n",
    "                                          np.array_str(diff, precision=5, suppress_small=True),\n",
    "                                          maxlen))\n",
    "\n",
    "        # check the diff vectors for the presence of any diff >= THRESHOLD\n",
    "        all_ok = True\n",
    "        THRESHOLD = 0.001\n",
    "        for diff_vector in diffs:\n",
    "            if np.any(diff_vector > THRESHOLD):\n",
    "                all_ok = False\n",
    "\n",
    "        # sum of the weights\n",
    "        sum_of_weights = np.sum(weights)\n",
    "        print()\n",
    "        print('        Max weight : {0:.3f}'.format(np.max(weights)))\n",
    "        print('Sum of the weights : {0:.3f}'.format(sum_of_weights))\n",
    "        print('  Population total : {0:.3f}'.format(TARGET_POPULATION))\n",
    "        print('        Difference : {0:.3f}'.format(abs(TARGET_POPULATION - sum_of_weights)))\n",
    "        print('\\nRaked PDFs differ from target PDFs by less than {0}: {1}'.format(int(THRESHOLD), all_ok))        \n",
    "        \n",
    "        # insert a column for the weights into the source dataframe\n",
    "        final_df = source_df.assign(Weight = weights)\n",
    "        \n",
    "        # construct output file name from the source file name\n",
    "        if IMPUTE_INCOME:\n",
    "            output_file = '{0}_imputed.csv'.format(state_abbrev.lower())\n",
    "        else:\n",
    "            output_file = '{0}.csv'.format(state_abbrev.lower())\n",
    "        output_filepath = os.path.join(OUTPUT_DIR, output_file)\n",
    "\n",
    "        # write output file with only the person_id and the weight\n",
    "        final_df[['person_id', 'Weight']].to_csv(output_filepath, index=False)\n",
    "        print('Wrote file \"{0}\".'.format(output_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score and rank the results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# collect results in a new dataframe\n",
    "result_df = pd.DataFrame(data=data, columns=['Max Weight', 'Min Cell', 'Model Index'])\n",
    "result_df = result_df.assign(**{'Score':scores})\n",
    "model_details = result_df['Model Index'].map(lambda x: MODELS[x])\n",
    "result_df = result_df.assign(**{'Model':model_details})\n",
    "result_df = result_df.sort_values(by=['Max Weight'])\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# build display dataframe\n",
    "sample_count = len(SOURCE_SAMPLES[0])\n",
    "display_df = result_df[['Max Weight', 'Min Cell', 'Score', 'Model']]\n",
    "\n",
    "print('AllOfUs results for source: {0}, target: {1}'.format(state_abbrev, state_abbrev))\n",
    "print('Source samples: {0}, Target state population: {1}, Imputed incomes: {2}'.format(sample_count,\n",
    "                                                                                       TARGET_POPULATION,\n",
    "                                                                                       IMPUTE_INCOME))\n",
    "print('Variables : {0}'.format([CODING.VAR_NAMES[enumvar] for enumvar in RAKEVARS]))\n",
    "print(tabulate(display_df, headers = 'keys', tablefmt = 'psql', floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print as csv data\n",
    "header = ','.join(display_df.columns)\n",
    "print(header)\n",
    "for index, row in display_df.iterrows():\n",
    "    wmax = '{0:.5f}'.format(row['Max Weight'])\n",
    "    min_cell = str(int(row['Min Cell']))\n",
    "    score = '{0:.5f}'.format(row['Score'])\n",
    "    model = str(row['Model'])   \n",
    "    \n",
    "    # extract only the two-way interactions from the model\n",
    "    iterator = re.finditer(r'\\[(?P<first>\\d), (?P<second>\\d)\\]', model)\n",
    "    \n",
    "    interactions = []\n",
    "    for match in iterator:\n",
    "        interaction = '[{0}{1}]'.format(match.group('first'), match.group('second'))\n",
    "        interactions.append(interaction)\n",
    "\n",
    "    if 0 == len(interactions):\n",
    "        interaction_str = '[]'\n",
    "    else:\n",
    "        interaction_str = ''.join(interactions)\n",
    "        \n",
    "    line = ','.join([wmax, min_cell, score, interaction_str])\n",
    "    print(line)         \n",
    "#    print('\\tinteractions: {0}'.format(interaction_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot weight components"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wts_max = []\n",
    "wts_m1 = []\n",
    "wts_m2 = []\n",
    "wts_99  = []\n",
    "wts_95  = []\n",
    "wts_90  = []   \n",
    "    \n",
    "for index, row in result_df.iterrows():    \n",
    "    model_index = row['Model Index']\n",
    "    weights_m = weight_map[model_index]    \n",
    "    sorted_weights = sorted(weights_m)\n",
    "  \n",
    "    wts_max.append(sorted_weights[-1])\n",
    "    wts_m1.append(sorted_weights[-5])\n",
    "    wts_m2.append(sorted_weights[-20])\n",
    "    wts_99.append(np.percentile(weights_m, 99))\n",
    "    wts_95.append(np.percentile(weights_m, 95))\n",
    "    wts_90.append(np.percentile(weights_m, 90))\n",
    "    \n",
    "plt.figure(figsize=(20,12))\n",
    "plt.plot(wts_max, label='Wt max', marker='o')\n",
    "plt.plot(wts_m1, label='Wt[-5]', marker='o')\n",
    "plt.plot(wts_m2, label='Wt[-20]', marker='o')\n",
    "plt.plot(wts_99, label='Wt 99%', marker='o')\n",
    "plt.plot(wts_95, label='Wt 95%', marker='o')\n",
    "plt.plot(wts_90, label='Wt 90%', marker='o')\n",
    "plt.xlabel('Max Weight Rank', fontsize=16)\n",
    "plt.ylabel('Weight Component', fontsize=16)\n",
    "plt.title('Selected Weights vs. Max Weight Rank ({0}-{1})'.format(SOURCE_STATE, TARGET_STATE), fontsize=20)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute percentage of population in upper weight components"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# N largest weights\n",
    "lim_data = [\n",
    "    #(4, [], 'Top 4'),\n",
    "    (8, [], 'Top 8'),\n",
    "    (16, [], 'Top 16'),\n",
    "    (32, [], 'Top 32'),\n",
    "#     (64, [], 'Top 64'),\n",
    "#     (128, [], 'Top 128'),\n",
    "#     (256, [], 'Top 256'),\n",
    "]\n",
    "\n",
    "# uppermost percentiles\n",
    "pct_data = [\n",
    "    (99, [], '99%+'),\n",
    "    (97, [], '97%+'),\n",
    "    (95, [], '95%+'),\n",
    "#     (93, [], '93%+'),x\n",
    "#     (90, [], '90%+'),\n",
    "]\n",
    "\n",
    "# pct of population >= cutoff weight\n",
    "max_data = [\n",
    "    (90, [], '90%'),\n",
    "    (85, [], '85%'),\n",
    "    (80, [], '80%'),\n",
    "    (75, [], '75%'),\n",
    "    #(70, [], '70%'),\n",
    "    #(60, [], '60%'),\n",
    "]\n",
    "\n",
    "for index, row in result_df.iterrows():\n",
    "    model_index = row['Model Index']\n",
    "    weights_m = weight_map[model_index]\n",
    "    sorted_weights = sorted(weights_m)\n",
    "    \n",
    "    for lim, arr, label in lim_data:\n",
    "        \n",
    "        if lim > len(sorted_weights):\n",
    "            break\n",
    "        \n",
    "        # the nth-from-the-max weight\n",
    "        wn = sorted_weights[-lim]\n",
    "\n",
    "        # extract all weights >= this value\n",
    "        samples = []\n",
    "        for w in weights_m:\n",
    "            assert w >= 0\n",
    "            if w >= wn:\n",
    "                samples.append(w)    \n",
    "             \n",
    "        # the weighted population in the top N weights\n",
    "        pop = np.sum(samples) #sorted_weights[-lim:])\n",
    "        # pct of the target population this represents\n",
    "        pct = 100.0 * (pop / TARGET_POPULATION)\n",
    "        arr.append(pct)\n",
    "    \n",
    "for index, row in result_df.iterrows():\n",
    "    model_index = row['Model Index']\n",
    "    weights_m = weight_map[model_index]\n",
    "    sorted_weights = sorted(weights_m)\n",
    "    \n",
    "    for lim, arr, label in pct_data:\n",
    "        wpct = np.percentile(weights_m, lim)\n",
    "        samples = []\n",
    "        for w in weights_m:\n",
    "            assert w >= 0\n",
    "            if w >= wpct:\n",
    "                samples.append(w)\n",
    "\n",
    "        pop = np.sum(samples)\n",
    "        pct = 100.0 * (pop / TARGET_POPULATION)\n",
    "        arr.append(pct)\n",
    "\n",
    "for index, row in result_df.iterrows():\n",
    "    model_index = row['Model Index']\n",
    "    weights_m = weight_map[model_index]\n",
    "    sorted_weights = sorted(weights_m)\n",
    "\n",
    "    for pct, arr, label in max_data:\n",
    "        wmax = np.max(weights_m)\n",
    "        cutoff = pct * 0.01 * wmax\n",
    "        # extract all weights >= cutoff\n",
    "        samples = []\n",
    "        for w in weights_m:\n",
    "            if w >= cutoff:\n",
    "                samples.append(w)\n",
    "        \n",
    "        # fraction of the population in these weights\n",
    "        pop_fraction = np.sum(samples) / TARGET_POPULATION\n",
    "        arr.append(pop_fraction * 100.0)        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "for lim, arr, label in lim_data:\n",
    "    if lim > len(sorted_weights):\n",
    "        continue\n",
    "    plt.plot(arr, label=label, marker='o')\n",
    "plt.xlabel('Max Weight Rank', fontsize=16)\n",
    "plt.ylabel('Percentage', fontsize=16)\n",
    "plt.title('Percentage of the Population Represented by the Max N Weights ({0}-{1})'.format(SOURCE_STATE,\n",
    "                                                                                           TARGET_STATE),\n",
    "          fontsize=20)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "print('Top 8 percentage for best model: {0:.3f}'.format(lim_data[0][1][0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "for lim, arr, label in max_data:\n",
    "    if lim > len(sorted_weights):\n",
    "        continue\n",
    "    plt.plot(arr, label=label, marker='o')\n",
    "plt.xlabel('Max Weight Rank', fontsize=16)\n",
    "plt.ylabel('Percentage', fontsize=16)\n",
    "plt.title('Percentage of the Population Greater than the Stated Pct of WMax ({0}-{1})'.format(SOURCE_STATE,\n",
    "                                                                                              TARGET_STATE),\n",
    "          fontsize=20)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute PDFs for All Variables and Compare with Target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the best model again, compute PDFs for all variables, and check for zero diffs with the target."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# let the model with the minimum wmax be the \"best\" result\n",
    "best_result = result_df.iloc[[0]]\n",
    "model_index = best_result['Model Index'][0]\n",
    "model = MODELS[model_index]\n",
    "\n",
    "raking_result_tuple = raking.rake(model,\n",
    "                                  SOURCE_SAMPLES,\n",
    "                                  TARGET_SAMPLES,\n",
    "                                  TARGET_WEIGHTS,\n",
    "                                  BIN_COUNTS,\n",
    "                                  TARGET_POPULATION)\n",
    "\n",
    "if raking_result_tuple is None:\n",
    "    print('\\n*** No acceptable raking model was found. ***')\n",
    "else:\n",
    "    weights, unraked, target, raked, fnorm, smallest_cell = raking_result_tuple\n",
    "    # compute PDFs\n",
    "    target_pdfs = {}\n",
    "    source_raked_pdfs = {}\n",
    "    source_unraked_pdfs = {}\n",
    "\n",
    "    # target pdfs\n",
    "    for q in range(len(RAKEVARS)):\n",
    "        target_pdfs[q] = pdf.to_pdf(BIN_COUNTS[q], TARGET_SAMPLES[q], weights=TARGET_WEIGHTS)\n",
    "\n",
    "    # unraked source pdfs\n",
    "    for q in range(len(RAKEVARS)):\n",
    "        source_unraked_pdfs[q] = pdf.to_pdf(BIN_COUNTS[q], SOURCE_SAMPLES[q], weights=None)\n",
    "\n",
    "    # raked source pdfs\n",
    "    for q in range(len(RAKEVARS)):\n",
    "        source_raked_pdfs[q] = pdf.to_pdf(BIN_COUNTS[q], SOURCE_SAMPLES[q], weights=weights)\n",
    "\n",
    "    print('\\nPDF diffs after raking for model {0}:\\n'.format(model))   \n",
    "\n",
    "    # diff between raked and target pdfs\n",
    "    diffs = []\n",
    "    for q in range(len(RAKEVARS)):\n",
    "        diff = source_raked_pdfs[q] - target_pdfs[q]\n",
    "        diffs.append(diff)\n",
    "        print('{0:>{2}} : {1}'.format(RAKEVAR_NAMES[q], \n",
    "                                      np.array_str(diff, precision=5, suppress_small=True),\n",
    "                                      maxlen))\n",
    "\n",
    "    # check the diff vectors for the presence of any diff > 0.01 (i.e. 1%)\n",
    "    all_ok = True\n",
    "    THRESHOLD = 0.01\n",
    "    for diff_vector in diffs:\n",
    "        if np.any(diff_vector > THRESHOLD):\n",
    "            all_ok = False\n",
    "    \n",
    "    # sum of the weights\n",
    "    sum_of_weights = np.sum(weights)\n",
    "    print()\n",
    "    print('        Max weight : {0:.3f}'.format(np.max(weights)))\n",
    "    print('Sum of the weights : {0:.3f}'.format(sum_of_weights))\n",
    "    print('  Population total : {0:.3f}'.format(TARGET_POPULATION))\n",
    "    print('        Difference : {0:.3f}'.format(abs(TARGET_POPULATION - sum_of_weights)))\n",
    "    print('\\nRaked PDFs differ from target PDFs by less than {0}%: {1}'.format(int(THRESHOLD * 100),\n",
    "                                                                               all_ok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 1D Distributions Before and After Raking"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plot_pdfs(source_unraked_pdfs, source_raked_pdfs, target_pdfs, SOURCE_STATE, TARGET_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Data on Distributions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Univariate Distributions for {0}-{1}, imputed incomes: {2}\\n'.format(SOURCE_STATE,\n",
    "                                                                            TARGET_STATE,\n",
    "                                                                            IMPUTE_INCOME))\n",
    "# display precision\n",
    "P = 5\n",
    "for q in range(0, len(source_unraked_pdfs)):\n",
    "    print('{0}: '.format(RAKEVAR_NAMES[q]))\n",
    "    print('\\tUnraked source PDF : {0}'.format(np.array_str(source_unraked_pdfs[q], precision=P)))\n",
    "    print('\\t  Raked source PDF : {0}'.format(np.array_str(source_raked_pdfs[q],   precision=P)))\n",
    "    print('\\t        Target PDF : {0}'.format(np.array_str(target_pdfs[q],         precision=P)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out weighted samples"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# insert a column for the weights into the source dataframe\n",
    "final_df = source_df.assign(Weight = weights)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# construct output file name from the source file name\n",
    "output_file = os.path.join(DATA_DIR, OUTPUT_FILE_NAME)\n",
    "\n",
    "# write output file\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print('Wrote file \"{0}\".'.format(output_file))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "my_dataframe = final_df   \n",
    "\n",
    "# Replace 'test.csv' with THE NAME of the file you're going to store in the bucket (don't delete the quotation marks)\n",
    "destination_filename = OUTPUT_FILE_NAME\n",
    "\n",
    "\n",
    "# save dataframe in a csv file in the same workspace as the notebook\n",
    "my_dataframe.to_csv(destination_filename, index=False)\n",
    "\n",
    "# get the bucket name\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# copy csv file to the bucket\n",
    "args = [\"gsutil\", \"cp\", f\"./{destination_filename}\", f\"{my_bucket}/data/\"]\n",
    "output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# print output from gsutil\n",
    "output.stderr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check zero-weight tuples"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "zero_df = final_df.loc[final_df['Weight'] == 0]\n",
    "zero_df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Found {0} zero-weight source tuples.'.format(len(zero_df)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# drop the weight col, not needed for these checks\n",
    "zero_df = zero_df.drop(columns=['Weight'])\n",
    "\n",
    "# convert df to set of tuples\n",
    "zero_wt_tup_set = set(zero_df.itertuples(index=False, name=None))\n",
    "\n",
    "# drop the weight col from the pums df\n",
    "pums_no_wt_df = pums_df.drop(columns=[PUMS_WEIGHT_COL])\n",
    "target_tup_set = set(pums_no_wt_df.itertuples(index=False, name=None))\n",
    "\n",
    "# these sets should be disjoint, meaning that none of the zero wt tuples occur in the PUMS population\n",
    "error_population = len(zero_wt_tup_set.intersection(target_tup_set))\n",
    "    \n",
    "# this number should be zero\n",
    "print('Number of zero-weight source samples found in the target population: {0}.'.format(error_population))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find extreme-weighted individuals"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def append_pums_population(df, col_name='PUMS Population'):\n",
    "    \"\"\"\n",
    "    Appends a column to the dataframe containing the PUMS weighted population for each row.\n",
    "    \"\"\"\n",
    "\n",
    "    # names of the source and target cols\n",
    "    age_s, age_t   = RAKE_MAP[CODING.Variables.AGE]\n",
    "    race_s, race_t = RAKE_MAP[CODING.Variables.RACE_ETH]\n",
    "    ins_s, ins_t   = RAKE_MAP[CODING.Variables.INSURANCE]\n",
    "    ed_s, ed_t     = RAKE_MAP[CODING.Variables.EDUCATION]\n",
    "    inc_s, inc_t   = RAKE_MAP[CODING.Variables.INCOME]\n",
    "    sex_s, sex_t   = RAKE_MAP[CODING.Variables.SEX]    \n",
    "    \n",
    "    pums_pop = []\n",
    "    for index, row in df.iterrows():\n",
    "        # next source row's values\n",
    "        age = row[age_s]\n",
    "        raceeth = row[race_s]\n",
    "        insurance = row[ins_s]\n",
    "        education = row[ed_s]\n",
    "        income = row[inc_s]\n",
    "        sex = row[sex_s]\n",
    "\n",
    "        # find all individuals in PUMS with this set of characteristics\n",
    "        tmp_df = pums_df[\n",
    "            (pums_df[age_t]==age)       &\n",
    "            (pums_df[race_t]==raceeth)  &\n",
    "            (pums_df[ins_t]==insurance) &\n",
    "            (pums_df[ed_t]==education)  &\n",
    "            (pums_df[inc_t]==income)    &\n",
    "            (pums_df[sex_t]==sex)\n",
    "        ]\n",
    "        \n",
    "        # total weight of these individuals\n",
    "        tmp_wts = tmp_df[PUMS_WEIGHT_COL].values\n",
    "        tmp_pop = np.sum(tmp_wts)\n",
    "        pums_pop.append(tmp_pop)        \n",
    "        #print('({0},{1},{2},{3},{4},{5} : {6})'.format(age, raceeth, insurance, education, income, sex, tmp_pop))\n",
    "        \n",
    "    df = df.assign(**{col_name:pums_pop})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "weight_samples = final_df['Weight'].values\n",
    "nonzero_weights = weight_samples[weight_samples > 0]\n",
    "min_wt = np.min(nonzero_weights)\n",
    "max_wt = np.max(nonzero_weights)\n",
    "print('Min nonzero weight : {0:.3f}'.format(min_wt))\n",
    "print('Max nonzero weight : {0:.3f}'.format(max_wt))\n",
    "print('             Ratio : {0:.3f}'.format(max_wt/min_wt))\n",
    "\n",
    "#sorted_weights = sorted(nonzero_weights, reverse=True)\n",
    "max_wt_df = final_df.sort_values(by=['Weight'], ascending=False)\n",
    "max_wt_df = max_wt_df.reset_index(drop=True)\n",
    "max_wt_df = max_wt_df.truncate(after=9)\n",
    "max_wt_df = append_pums_population(max_wt_df)\n",
    "print('\\nMax-weighted individuals, {0}-{1}'.format(SOURCE_STATE, TARGET_STATE))\n",
    "display(max_wt_df.drop(columns=['person_id']))\n",
    "\n",
    "min_wt_df = final_df.sort_values(by=['Weight'])\n",
    "min_wt_df = min_wt_df.reset_index(drop=True)\n",
    "# find number of weights less than 1\n",
    "tmp_df = min_wt_df[min_wt_df['Weight'] < 1.0]\n",
    "wts_lt_1 = tmp_df.shape[0]\n",
    "min_wt_df = min_wt_df.truncate(after=9)\n",
    "min_wt_df = append_pums_population(min_wt_df)\n",
    "print('Min-weighted individuals, {0}-{1}'.format(SOURCE_STATE, TARGET_STATE))\n",
    "display(min_wt_df.drop(columns=['person_id']))\n",
    "\n",
    "print('Max-weighted individuals in state {0}'.format(TARGET_STATE))\n",
    "max_wt_df_2 = final_df[final_df['STATE'] == TARGET_STATE].sort_values(by=['Weight'], ascending=False)\n",
    "max_wt_df_2 = max_wt_df_2.reset_index(drop=True)\n",
    "max_wt_df_2 = max_wt_df_2.truncate(after=9)\n",
    "max_wt_df_2 = append_pums_population(max_wt_df_2)\n",
    "display(max_wt_df_2.drop(columns=['person_id']))\n",
    "\n",
    "print('Min-weighted individuals in state {0}'.format(TARGET_STATE))\n",
    "min_wt_df_2 = final_df[final_df['STATE'] == TARGET_STATE].sort_values(by=['Weight'])\n",
    "min_wt_df_2 = min_wt_df_2.reset_index(drop=True)\n",
    "min_wt_df_2 = min_wt_df_2.truncate(after=9)\n",
    "min_wt_df_2 = append_pums_population(min_wt_df_2)\n",
    "display(min_wt_df_2.drop(columns=['person_id']))\n",
    "\n",
    "print('Number of weights less than 1.0: {0}'.format(wts_lt_1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# number with the minimum weight\n",
    "sorted_weights = sorted(final_df['Weight'].values)\n",
    "tmp_df = final_df[final_df['Weight']==sorted_weights[0]]\n",
    "print('Number with the minumum weight: {0}'.format(tmp_df.shape[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## hist of non-zero weights\n",
    "def plot_weight_distribution(plot_weights, title):\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    bin_values, bin_edges, patches = plt.hist(plot_weights, bins=100, edgecolor='k', alpha=0.5)\n",
    "    plt.xlabel('$w_i$', fontsize=16)\n",
    "    plt.ylabel('Count', fontsize=16)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.yscale('log')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return bin_values, bin_edges"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "raked_weights = nonzero_weights\n",
    "bin_values, bin_edges = plot_weight_distribution(raked_weights,\n",
    "                                                 'Weight Distribution ({0}-{1})'.format(SOURCE_STATE, TARGET_STATE))\n",
    "print('Min wt: {0:.3f}, max wt: {1:.3f}, ratio: {2:.3f}, wts less than 1: {3}'.format(min_wt,\n",
    "                                                                                      max_wt,\n",
    "                                                                                      max_wt/min_wt,\n",
    "                                                                                      wts_lt_1))\n",
    "\n",
    "# print bin info for leftmost bins\n",
    "print('\\n       BIN       : COUNT')\n",
    "for i, edge in enumerate(bin_edges[:10]):\n",
    "    edge_l = '{0:.3f}'.format(bin_edges[i])\n",
    "    edge_r = '{0:.3f}'.format(bin_edges[i+1])\n",
    "    bin_count = bin_values[i]\n",
    "    print('[{0:>7}, {1:>7}) : {2}'.format(edge_l, edge_r, bin_count))\n",
    "#target_weights = pums_df['PWGTP'].values\n",
    "#plot_weight_distribution(target_weights, 'PUMS Weight Distribution ({0})'.format(TARGET_STATE))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## common tuples\n",
    "# drop the weight column from the PUMS dataframe, so that only the variable cols remain\n",
    "target_no_wt_df = pums_df.drop(columns=['PWGTP'])\n",
    "\n",
    "# drop the state column from the source_df\n",
    "source_no_st_df = source_df.drop(columns=['STATE'])\n",
    "\n",
    "# get set of (unique) target tuples\n",
    "target_tuples = set(target_no_wt_df.itertuples(index=False, name=None))\n",
    "\n",
    "# get set of (unique) source tuples\n",
    "source_tuples = set(source_no_st_df.itertuples(index=False, name=None))\n",
    "\n",
    "# intersection\n",
    "shared_tuples = source_tuples.intersection(target_tuples)\n",
    "\n",
    "print('  Unique PUMS tuples : {0}'.format(len(target_tuples)))\n",
    "print('Unique source tuples : {0}'.format(len(source_tuples)))\n",
    "print('       Shared tuples : {0}'.format(len(shared_tuples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find PUMS population for specific cells"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "age_s, age_t   = RAKE_MAP[CODING.Variables.AGE]\n",
    "race_s, race_t = RAKE_MAP[CODING.Variables.RACE_ETH]\n",
    "ins_s, ins_t   = RAKE_MAP[CODING.Variables.INSURANCE]\n",
    "ed_s, ed_t     = RAKE_MAP[CODING.Variables.EDUCATION]\n",
    "inc_s, inc_t   = RAKE_MAP[CODING.Variables.INCOME]\n",
    "sex_s, sex_t   = RAKE_MAP[CODING.Variables.SEX]  \n",
    "\n",
    "age_code  = 5\n",
    "race_code = 1\n",
    "ins_code  = 0\n",
    "edu_code  = 1\n",
    "inc_code  = 3\n",
    "sex_code  = 1\n",
    "\n",
    "tmp_df = pums_usa_df[\n",
    "    (pums_usa_df[age_t] ==age_code)  &\n",
    "    (pums_usa_df[race_t]==race_code) &\n",
    "    (pums_usa_df[ins_t] ==ins_code)  &\n",
    "    (pums_usa_df[ed_t]  ==edu_code)  &\n",
    "    (pums_usa_df[inc_t] ==inc_code)  &\n",
    "    (pums_usa_df[sex_t] ==sex_code)\n",
    "]\n",
    "\n",
    "states = [INV_STATE_CODE_MAP[code] for code in tmp_df['ST'].unique()]\n",
    "print('Found {0} entries in {1} states '.format(tmp_df.shape[0], len(states)))\n",
    "tmp_pop = np.sum(tmp_df[PUMS_WEIGHT_COL].values)\n",
    "print('Total weighted population: {0}'.format(tmp_pop))\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#source_df['EDUCATION_GROUPING'].value_counts()\n",
    "tmp_df = source_df[(source_df['INCOME_NUM']==3) & (source_df['sg']==1)]\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pums_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp_df = pums_df[pums_df['RaceEth']==1]\n",
    "#tmp_df = pums_df[(pums_df['Income']==3) & (pums_df['Sex']==1)]\n",
    "#tmp_df\n",
    "np.sum(tmp_df['PWGTP'].values)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# This snippet assumes you run setup first\n",
    "\n",
    "# This code saves your dataframe into a csv file in a \"data\" folder in Google Bucket\n",
    "\n",
    "# Replace df with THE NAME OF YOUR DATAFRAME\n",
    "my_dataframe = df   \n",
    "\n",
    "# Replace 'test.csv' with THE NAME of the file you're going to store in the bucket (don't delete the quotation marks)\n",
    "destination_filename = 'test.csv'\n",
    "\n",
    "########################################################################\n",
    "##\n",
    "################# DON'T CHANGE FROM HERE ###############################\n",
    "##\n",
    "########################################################################\n",
    "\n",
    "# save dataframe in a csv file in the same workspace as the notebook\n",
    "my_dataframe.to_csv(destination_filename, index=False)\n",
    "\n",
    "# get the bucket name\n",
    "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
    "\n",
    "# copy csv file to the bucket\n",
    "args = [\"gsutil\", \"cp\", f\"./{destination_filename}\", f\"{my_bucket}/data/\"]\n",
    "output = subprocess.run(args, capture_output=True)\n",
    "\n",
    "# print output from gsutil\n",
    "output.stderr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate MaxWt Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_wt(filepath):\n",
    "    \n",
    "    max_wt = 0.0\n",
    "    with open(filepath, 'rt') as infile:\n",
    "        for lineno, line in enumerate(infile):\n",
    "            if 0 == lineno:\n",
    "                continue\n",
    "            text = line.strip()\n",
    "            person_id, weight = text.split(',')\n",
    "            weight = float(weight)\n",
    "            if weight > max_wt:\n",
    "                max_wt = weight\n",
    "    return max_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDERS = ['weighting_2019', 'weight_as_2019', 'weighting_2020', 'weight_as_2020']\n",
    "\n",
    "data_2019 = []\n",
    "folder = os.path.join('results', 'weighting_2019', 'model_2')\n",
    "for f in os.listdir(folder):\n",
    "    # get the state abbrev\n",
    "    state_abbrev = f[6:8]\n",
    "    filepath = os.path.join(folder, f)\n",
    "    max_wt_2019 = get_max_wt(filepath)\n",
    "    data_2019.append( (state_abbrev, max_wt_2019))\n",
    "data_2019 = sorted(data_2019, key=lambda x: x[0])\n",
    "\n",
    "data_2020 = []\n",
    "folder = os.path.join('results', 'weighting_2020', 'model_2')\n",
    "for f in os.listdir(folder):\n",
    "    state_abbrev = f[6:8]\n",
    "    filepath = os.path.join(folder, f)\n",
    "    max_wt_2020 = get_max_wt(filepath)\n",
    "    data_2020.append( (state_abbrev,  max_wt_2020))\n",
    "data_2020 = sorted(data_2020, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2_2019 = []\n",
    "folder = os.path.join('results', 'weight_as_2019')\n",
    "for f in os.listdir(folder):\n",
    "    state_abbrev = f[0:2]\n",
    "    filepath = os.path.join(folder, f)\n",
    "    max_wt_2019 = get_max_wt(filepath)\n",
    "    data_2_2019.append( (state_abbrev, max_wt_2019))\n",
    "data_2_2019 = sorted(data_2_2019, key=lambda x: x[0])\n",
    "\n",
    "data_2_2020 = []\n",
    "folder = os.path.join('results', 'weight_as_2020')\n",
    "for f in os.listdir(folder):\n",
    "    state_abbrev = f[0:2]\n",
    "    filepath = os.path.join(folder, f)\n",
    "    max_wt_2020 = get_max_wt(filepath)\n",
    "    data_2_2020.append( (state_abbrev, max_wt_2020))\n",
    "data_2_2020 = sorted(data_2_2020, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_2019) == len(data_2_2019) == len(data_2020) == len(data_2_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, tup in enumerate(data_2019):\n",
    "    state = tup[0]\n",
    "    wt_2019 = tup[1]\n",
    "    wt_2020 = data_2020[i][1]\n",
    "    wt_as_2019 = data_2_2019[i][1]\n",
    "    wt_as_2020 = data_2_2020[i][1]\n",
    "    data.append( (state, wt_2019, wt_as_2019, wt_2020, wt_as_2020))\n",
    "    #print('{0} : {1}, {2}, {3}, {4}'.format(state, wt_2019, wt_as_2019, wt_2020, wt_as_2020))\n",
    "    \n",
    "df = pd.DataFrame(data, columns=['State', 'MAX_WT 2019', 'MAX_WTAS 2019', 'MAX_WT 2020', 'MAX_WTAS 2020'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'State', 'MAX_WT 2019', 'MAX_WTAS 2019', 'MAX_WT 2020', 'MAX_WTAS 2020']\n",
    "\n",
    "# print as csv data\n",
    "header = ','.join(df.columns)\n",
    "print(header)\n",
    "for index, row in df.iterrows():\n",
    "    state = row['State']\n",
    "    max_wt_2019 = str(row['MAX_WT 2019'])\n",
    "    max_wtas_2019 = str(row['MAX_WTAS 2019'])\n",
    "    max_wt_2020 = str(row['MAX_WT 2020'])\n",
    "    max_wtas_2020 = str(row['MAX_WTAS 2020'])\n",
    "    line = ','.join([state, max_wt_2019, max_wtas_2019, max_wt_2020, max_wtas_2020])\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
