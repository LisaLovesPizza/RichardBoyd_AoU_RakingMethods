{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6cbe55",
   "metadata": {},
   "source": [
    "## PUMS Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9b886",
   "metadata": {},
   "source": [
    "Download all PUMS person-level state files for a given year, extract relevant columns, concatenate the data for all states into a single USA dataframe, and write result file to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33881f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b95eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year of PUMS data\n",
    "YEAR = 2019\n",
    "\n",
    "# URL of the FTP site for the PUMS 2020 1-year data files\n",
    "if YEAR <= 2019:\n",
    "    # these years have production weights\n",
    "    PUMS_URL = 'https://www2.census.gov/programs-surveys/acs/data/pums/{0}/1-Year/'.format(YEAR)\n",
    "else:\n",
    "    # experimental weights because of the Covid pandemic\n",
    "    PUMS_URL = 'https://www2.census.gov/programs-surveys/acs/experimental/{0}/data/pums/1-Year/'.format(YEAR)\n",
    "\n",
    "# output file\n",
    "OUTPUT_FILE = 'pums_usa_{0}.csv'.format(YEAR)\n",
    "\n",
    "# These are the two-letter abbreviations used by the pums files.\n",
    "# Each person-level filename has this form: \"csv_pxy.zip\", where\n",
    "# 'xy' represents the two-letter abbreviation for a jurisdiction\n",
    "# in this list:\n",
    "JURISDICTIONS = [\n",
    "    'ak', # Alaska\n",
    "    'al', # Alabama\n",
    "    'ar', # Arkansas\n",
    "    'az', # Arizona\n",
    "    'ca', # California\n",
    "    'co', # Colorado\n",
    "    'ct', # Connecticut\n",
    "    'dc', # District of Columbia *** (PUMS includes DC also)\n",
    "    'de', # Delaware\n",
    "    'fl', # Florida\n",
    "    'ga', # Georgia\n",
    "    'hi', # Hawaii\n",
    "    'ia', # Iowa\n",
    "    'id', # Idaho\n",
    "    'il', # Illinois\n",
    "    'in', # Indiana\n",
    "    'ks', # Kansas\n",
    "    'ky', # Kentucky\n",
    "    'la', # Louisiana\n",
    "    'ma', # Massachusetts\n",
    "    'md', # Maryland\n",
    "    'me', # Maine\n",
    "    'mi', # Michigan\n",
    "    'mn', # Minnesota\n",
    "    'mo', # Missouri\n",
    "    'ms', # Mississippi\n",
    "    'mt', # Montana\n",
    "    'nc', # North Carolina\n",
    "    'nd', # North Dakota\n",
    "    'ne', # Nebraska\n",
    "    'nh', # New Hampshire\n",
    "    'nj', # New Jersey\n",
    "    'nm', # New Mexico\n",
    "    'nv', # Nevada\n",
    "    'ny', # New York\n",
    "    'oh', # Ohio\n",
    "    'ok', # Oklahoma\n",
    "    'or', # Oregon\n",
    "    'pa', # Pennsylvania\n",
    "    'ri', # Rhode Island\n",
    "    'sc', # South Carolina\n",
    "    'sd', # South Dakota\n",
    "    'tn', # Tennessee\n",
    "    'tx', # Texas\n",
    "    'ut', # Utah\n",
    "    'va', # Virginia\n",
    "    'vt', # Vermont\n",
    "    'wa', # Washington\n",
    "    'wi', # Wisconsin\n",
    "    'wv', # West Virginia\n",
    "    'wy', # Wyoming\n",
    "]\n",
    "\n",
    "# should have all 50 states + DC\n",
    "assert 51 == len(set(JURISDICTIONS))\n",
    "\n",
    "# columns to extract from each file\n",
    "KEEP_COLS = [\n",
    "    'ST',       # state code based on 2010 census definitions\n",
    "    'SERIALNO', # essentially the household ID\n",
    "    'SPORDER',  # unique ID for each person within a given household\n",
    "    'AGEP',     # numeric age, 0..99\n",
    "    'ADJINC',   # income adjustment factor to constant dollars\n",
    "    'PWGTP',    # person-level weight\n",
    "    'HISP',     # recoded detailed Hispanic origin (01 == not Spanish/Hispanic/Latino)\n",
    "    'RAC1P',    # detailed race code\n",
    "    'SCHL',     # educational attainment\n",
    "    'HICOV',    # health insurance recode\n",
    "    'PINCP',    # total person's income (use ADJINC to adjust to constant dollars)\n",
    "    'SEX',      # sex (at birth, presumably)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14b1dc",
   "metadata": {},
   "source": [
    "#### Download all zip files for all jurisdictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050a19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded csv_pak.zip ( 1/51)\n",
      "Downloaded csv_pal.zip ( 2/51)\n",
      "Downloaded csv_par.zip ( 3/51)\n",
      "Downloaded csv_paz.zip ( 4/51)\n",
      "Downloaded csv_pca.zip ( 5/51)\n",
      "Downloaded csv_pco.zip ( 6/51)\n",
      "Downloaded csv_pct.zip ( 7/51)\n",
      "Downloaded csv_pdc.zip ( 8/51)\n",
      "Downloaded csv_pde.zip ( 9/51)\n",
      "Downloaded csv_pfl.zip (10/51)\n",
      "Downloaded csv_pga.zip (11/51)\n",
      "Downloaded csv_phi.zip (12/51)\n",
      "Downloaded csv_pia.zip (13/51)\n",
      "Downloaded csv_pid.zip (14/51)\n",
      "Downloaded csv_pil.zip (15/51)\n",
      "Downloaded csv_pin.zip (16/51)\n",
      "Downloaded csv_pks.zip (17/51)\n",
      "Downloaded csv_pky.zip (18/51)\n",
      "Downloaded csv_pla.zip (19/51)\n",
      "Downloaded csv_pma.zip (20/51)\n",
      "Downloaded csv_pmd.zip (21/51)\n",
      "Downloaded csv_pme.zip (22/51)\n",
      "Downloaded csv_pmi.zip (23/51)\n",
      "Downloaded csv_pmn.zip (24/51)\n",
      "Downloaded csv_pmo.zip (25/51)\n",
      "Downloaded csv_pms.zip (26/51)\n",
      "Downloaded csv_pmt.zip (27/51)\n",
      "Downloaded csv_pnc.zip (28/51)\n",
      "Downloaded csv_pnd.zip (29/51)\n",
      "Downloaded csv_pne.zip (30/51)\n",
      "Downloaded csv_pnh.zip (31/51)\n",
      "Downloaded csv_pnj.zip (32/51)\n",
      "Downloaded csv_pnm.zip (33/51)\n",
      "Downloaded csv_pnv.zip (34/51)\n",
      "Downloaded csv_pny.zip (35/51)\n",
      "Downloaded csv_poh.zip (36/51)\n",
      "Downloaded csv_pok.zip (37/51)\n",
      "Downloaded csv_por.zip (38/51)\n",
      "Downloaded csv_ppa.zip (39/51)\n",
      "Downloaded csv_pri.zip (40/51)\n",
      "Downloaded csv_psc.zip (41/51)\n",
      "Downloaded csv_psd.zip (42/51)\n",
      "Downloaded csv_ptn.zip (43/51)\n",
      "Downloaded csv_ptx.zip (44/51)\n",
      "Downloaded csv_put.zip (45/51)\n",
      "Downloaded csv_pva.zip (46/51)\n",
      "Downloaded csv_pvt.zip (47/51)\n",
      "Downloaded csv_pwa.zip (48/51)\n",
      "Downloaded csv_pwi.zip (49/51)\n",
      "Downloaded csv_pwv.zip (50/51)\n",
      "Downloaded csv_pwy.zip (51/51)\n"
     ]
    }
   ],
   "source": [
    "# folder to hold all state files\n",
    "STATE_ZIP_FOLDER = 'state_zip_files'\n",
    "if not os.path.exists(STATE_ZIP_FOLDER):\n",
    "    os.makedirs(STATE_ZIP_FOLDER)\n",
    "    \n",
    "num_downloaded = 0\n",
    "for i,abbrev in enumerate(JURISDICTIONS):\n",
    "    # source PUMS file\n",
    "    pums_file = 'csv_p{0}.zip'.format(abbrev)\n",
    "    # destination file\n",
    "    output_file = os.path.join(STATE_ZIP_FOLDER, pums_file)\n",
    "    with open(output_file, 'wb') as outfile:\n",
    "        url = urljoin(PUMS_URL, pums_file)\n",
    "        zip_data = requests.get(url, stream=True).content\n",
    "        outfile.write(zip_data)\n",
    "        print('Downloaded {0} ({1:2d}/{2})'.format(pums_file, i+1, len(JURISDICTIONS)))\n",
    "        num_downloaded += 1\n",
    "        \n",
    "assert len(JURISDICTIONS) == num_downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3de0a3",
   "metadata": {},
   "source": [
    "#### Unzip and process each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9feba191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_individuals(df):\n",
    "    \"\"\"\n",
    "    Form an identifier from the SERIALNO and SPORDER fields. The SERIALNO field is a household or family ID,\n",
    "    and the SPORDER field is the person ID within the household or family. The set of all combinations of\n",
    "    these fields for a given state should be a unique identifier for the people of that state.\n",
    "    \"\"\"\n",
    "    \n",
    "    id_set = set()\n",
    "    for index, row in df.iterrows():\n",
    "        # household/family ID\n",
    "        serialno = row['SERIALNO']\n",
    "        # person id within the household\n",
    "        person = row['SPORDER']\n",
    "        key = '{0}|{1}'.format(serialno, person)\n",
    "        id_set.add(key)\n",
    "    unique_individuals = len(id_set)\n",
    "    return unique_individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac74ad10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing csv_pin.zip (1/51)\n",
      "Processing csv_phi.zip (2/51)\n",
      "Processing csv_psc.zip (3/51)\n",
      "Processing csv_pil.zip (4/51)\n",
      "Processing csv_pks.zip (5/51)\n",
      "Processing csv_psd.zip (6/51)\n",
      "Processing csv_pga.zip (7/51)\n",
      "Processing csv_pla.zip (8/51)\n",
      "Processing csv_pme.zip (9/51)\n",
      "Processing csv_put.zip (10/51)\n",
      "Processing csv_pms.zip (11/51)\n",
      "Processing csv_pnh.zip (12/51)\n",
      "Processing csv_pmd.zip (13/51)\n",
      "Processing csv_pco.zip (14/51)\n",
      "Processing csv_par.zip (15/51)\n",
      "Processing csv_pnj.zip (16/51)\n",
      "Processing csv_pwi.zip (17/51)\n",
      "Processing csv_pmt.zip (18/51)\n",
      "Processing csv_pok.zip (19/51)\n",
      "Processing csv_pny.zip (20/51)\n",
      "Processing csv_poh.zip (21/51)\n",
      "Processing csv_pwy.zip (22/51)\n",
      "Processing csv_pma.zip (23/51)\n",
      "Processing csv_pnm.zip (24/51)\n",
      "Processing csv_ptn.zip (25/51)\n",
      "Processing csv_ptx.zip (26/51)\n",
      "Processing csv_por.zip (27/51)\n",
      "Processing csv_pnv.zip (28/51)\n",
      "Processing csv_pal.zip (29/51)\n",
      "Processing csv_pnc.zip (30/51)\n",
      "Processing csv_pmo.zip (31/51)\n",
      "Processing csv_pwa.zip (32/51)\n",
      "Processing csv_pwv.zip (33/51)\n",
      "Processing csv_pmn.zip (34/51)\n",
      "Processing csv_paz.zip (35/51)\n",
      "Processing csv_pca.zip (36/51)\n",
      "Processing csv_pva.zip (37/51)\n",
      "Processing csv_pmi.zip (38/51)\n",
      "Processing csv_pne.zip (39/51)\n",
      "Processing csv_pvt.zip (40/51)\n",
      "Processing csv_pnd.zip (41/51)\n",
      "Processing csv_pct.zip (42/51)\n",
      "Processing csv_pak.zip (43/51)\n",
      "Processing csv_pky.zip (44/51)\n",
      "Processing csv_pdc.zip (45/51)\n",
      "Processing csv_pid.zip (46/51)\n",
      "Processing csv_pia.zip (47/51)\n",
      "Processing csv_pde.zip (48/51)\n",
      "Processing csv_ppa.zip (49/51)\n",
      "Processing csv_pri.zip (50/51)\n",
      "Processing csv_pfl.zip (51/51)\n",
      "\n",
      "Processed 51 files.\n",
      "No corrupted zip files found.\n"
     ]
    }
   ],
   "source": [
    "corrupted = []\n",
    "dataframes = []\n",
    "\n",
    "file_index = 0\n",
    "for f in os.listdir(STATE_ZIP_FOLDER):\n",
    "    filename = os.path.join(STATE_ZIP_FOLDER, f)\n",
    "    if not filename.endswith('.zip'):\n",
    "        continue\n",
    "    print('Processing {0} ({1}/{2})'.format(f, file_index+1, num_downloaded))\n",
    "    try:\n",
    "        with zipfile.ZipFile(filename, 'r') as zf:\n",
    "            members = zf.infolist()\n",
    "            for m in members:\n",
    "                member_filename = m.filename\n",
    "                if member_filename.endswith('.pdf'):\n",
    "                    # skip documentation\n",
    "                    continue\n",
    "                with zf.open(member_filename) as csvfile:\n",
    "                    df = pd.read_csv(csvfile)\n",
    "                    # extract desired columns\n",
    "                    df = df[KEEP_COLS].copy()\n",
    "                    # count unique individuals\n",
    "                    uniques = count_unique_individuals(df)\n",
    "                    # should have a single unique individual in each row of the dataframe\n",
    "                    assert df.shape[0] == uniques\n",
    "                    dataframes.append(df)\n",
    "    except zipfile.BadZipFile:\n",
    "        corrupted.append(filename)\n",
    "    file_index += 1\n",
    "        \n",
    "print('\\nProcessed {0} files.'.format(file_index))\n",
    "num_corrupted = len(corrupted)\n",
    "if 0 == num_corrupted:\n",
    "    print('No corrupted zip files found.')\n",
    "else:\n",
    "    print('Found these corrupted zip files: {0}'.format(corrupted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dcc2820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ST</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>SPORDER</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>ADJINC</th>\n",
       "      <th>PWGTP</th>\n",
       "      <th>HISP</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>HICOV</th>\n",
       "      <th>PINCP</th>\n",
       "      <th>SEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>2019GQ0000001</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1010145</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>2019GQ0000007</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1010145</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>2019GQ0000033</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1010145</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7400.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>2019GQ0000125</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1010145</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>2019GQ0000276</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1010145</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205289</th>\n",
       "      <td>12</td>\n",
       "      <td>2019HU1412413</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1010145</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205290</th>\n",
       "      <td>12</td>\n",
       "      <td>2019HU1412413</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1010145</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205291</th>\n",
       "      <td>12</td>\n",
       "      <td>2019HU1412447</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>1010145</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205292</th>\n",
       "      <td>12</td>\n",
       "      <td>2019HU1412447</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>1010145</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205293</th>\n",
       "      <td>12</td>\n",
       "      <td>2019HU1412449</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1010145</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3239553 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ST       SERIALNO  SPORDER  AGEP   ADJINC  PWGTP  HISP  RAC1P  SCHL  \\\n",
       "0       18  2019GQ0000001        1    35  1010145     71     1      1  17.0   \n",
       "1       18  2019GQ0000007        1    21  1010145     24     1      6  19.0   \n",
       "2       18  2019GQ0000033        1    21  1010145     15     1      1  19.0   \n",
       "3       18  2019GQ0000125        1    19  1010145     43     1      1  15.0   \n",
       "4       18  2019GQ0000276        1    18  1010145     87     1      1  18.0   \n",
       "...     ..            ...      ...   ...      ...    ...   ...    ...   ...   \n",
       "205289  12  2019HU1412413        4     6  1010145     76     1      1   4.0   \n",
       "205290  12  2019HU1412413        5     3  1010145     74     1      1   1.0   \n",
       "205291  12  2019HU1412447        1    59  1010145     56     1      1  22.0   \n",
       "205292  12  2019HU1412447        2    57  1010145     57     1      1  20.0   \n",
       "205293  12  2019HU1412449        1    52  1010145     45     1      6  22.0   \n",
       "\n",
       "        HICOV     PINCP  SEX  \n",
       "0           2   63000.0    1  \n",
       "1           1    2000.0    2  \n",
       "2           1    7400.0    2  \n",
       "3           2       0.0    1  \n",
       "4           1       0.0    1  \n",
       "...       ...       ...  ...  \n",
       "205289      1       NaN    1  \n",
       "205290      1       NaN    1  \n",
       "205291      1    5000.0    1  \n",
       "205292      2  130000.0    2  \n",
       "205293      1  120000.0    2  \n",
       "\n",
       "[3239553 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate the dataframes\n",
    "usa_df = pd.concat(dataframes)\n",
    "usa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567b3090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ST</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>SPORDER</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>ADJINC</th>\n",
       "      <th>PWGTP</th>\n",
       "      <th>HISP</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>HICOV</th>\n",
       "      <th>PINCP</th>\n",
       "      <th>SEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43847</th>\n",
       "      <td>1</td>\n",
       "      <td>2019HU1259536</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1010145</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32613</th>\n",
       "      <td>1</td>\n",
       "      <td>2019HU0915454</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1010145</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32614</th>\n",
       "      <td>1</td>\n",
       "      <td>2019HU0915504</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1010145</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>94600.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32615</th>\n",
       "      <td>1</td>\n",
       "      <td>2019HU0915555</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>1010145</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37100.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32616</th>\n",
       "      <td>1</td>\n",
       "      <td>2019HU0915555</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>1010145</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>56</td>\n",
       "      <td>2019HU1000576</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>1010145</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>56</td>\n",
       "      <td>2019HU1000576</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1010145</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>56</td>\n",
       "      <td>2019HU1000791</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>1010145</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13900.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4217</th>\n",
       "      <td>56</td>\n",
       "      <td>2019HU0999472</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1010145</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>33700.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>56</td>\n",
       "      <td>2019HU0769830</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1010145</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3239553 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ST       SERIALNO  SPORDER  AGEP   ADJINC  PWGTP  HISP  RAC1P  SCHL  \\\n",
       "43847   1  2019HU1259536        4    10  1010145    185     1      2   7.0   \n",
       "32613   1  2019HU0915454        2    17  1010145     77     1      1  13.0   \n",
       "32614   1  2019HU0915504        1    46  1010145     23     2      1  19.0   \n",
       "32615   1  2019HU0915555        1    69  1010145     71     1      1  16.0   \n",
       "32616   1  2019HU0915555        2    72  1010145     53     1      1  21.0   \n",
       "...    ..            ...      ...   ...      ...    ...   ...    ...   ...   \n",
       "4223   56  2019HU1000576        2    56  1010145    134     1      1  18.0   \n",
       "4224   56  2019HU1000576        3    17  1010145    141     1      1  14.0   \n",
       "4225   56  2019HU1000791        1    66  1010145     24     1      1  16.0   \n",
       "4217   56  2019HU0999472        1    33  1010145     48     2      1  21.0   \n",
       "3275   56  2019HU0769830        1    49  1010145    103     1      1  14.0   \n",
       "\n",
       "       HICOV    PINCP  SEX  \n",
       "43847      1      NaN    1  \n",
       "32613      1  10000.0    2  \n",
       "32614      1  94600.0    1  \n",
       "32615      1  37100.0    2  \n",
       "32616      1   8400.0    1  \n",
       "...      ...      ...  ...  \n",
       "4223       1      0.0    2  \n",
       "4224       1      0.0    1  \n",
       "4225       1  13900.0    2  \n",
       "4217       1  33700.0    2  \n",
       "3275       2  58000.0    1  \n",
       "\n",
       "[3239553 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by state\n",
    "usa_df = usa_df.sort_values(by=['ST'])\n",
    "usa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d3ee3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ST values: 51\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique ST values: {0}'.format(len(set(usa_df['ST'].values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e96e3",
   "metadata": {},
   "source": [
    "#### Write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af0701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file \"pums_usa_2019.csv\".\n"
     ]
    }
   ],
   "source": [
    "usa_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print('Wrote file \"{0}\".'.format(OUTPUT_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2320a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
